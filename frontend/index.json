[{"content":"","externalUrl":null,"permalink":"/projects/202501-devops-challenge-ccc/-5/","section":"Projects","summary":"","title":"5","type":"Projects"},{"content":"","externalUrl":null,"permalink":"/projects/202501-devops-challenge-ccc/-4/","section":"Projects","summary":"","title":"4","type":"Projects"},{"content":" AWS Lambda AWS Glue S3 bucket Amazon Athena SportsData API null Overview # The challenge focuses on building a data lake on AWS to store and analyze NBA data.\nThe data lake ingests data from a sports API, stores it in a scalable and cost-effective manner on S3, catalogs it using AWS Glue, and enables querying and analysis using Amazon Athena. Objectives # Ingest data: Fetch NBA data from a sports API (e.g., SportsDataIO). Store data: Design an S3 bucket structure to house raw and processed data efficiently. Catalog data: Use AWS Glue to create a data catalog for easy discovery and querying. Query data: Use Amazon Athena to run queries on the data for analysis. Architecture # graph TD subgraph \"AWS Services\" A[S3 Bucket] --\u003e|Store NBA Data| B[Glue Catalog] B --\u003e|Schema and Metadata| C[Athena] C --\u003e|Query Data| D[Analytics \u0026 Insights] end subgraph \"Data Flow\" G[SportsData.io API] --\u003e|Fetch NBA Data| A A --\u003e|Store Raw \u0026 Processed Data| B D --\u003e|Generate Reports| E[SQL Queries] end subgraph \"Results\" D --\u003e|Results in S3| A end Implementation Details # Data Ingestion:An AWS Lambda function is triggered (e.g., by EventBridge on a schedule) to fetch NBA data from the chosen API.The Lambda function processes the data and stores it as raw JSON files in an S3 bucket (e.g., s3://your-bucket-name/raw-data/). Data Storage (S3):An S3 bucket is set up to store the NBA data. A directory structure is created within the bucket to organize data based on source, date, and processing level (raw, processed): s3://your-bucket-name/ ├── raw-data/ │ └── nba/ │ └── year=2024/ │ └── month=10/ │ └── day=26/ │ └── data.json └── processed-data/ └── ... Data Catalog (AWS Glue):AWS Glue crawlers are used to automatically discover and catalog the data stored in S3.A Glue database is created to represent the NBA data.Glue tables are defined, mapping the schema of the data to columns that can be queried. Querying and Analysis (Amazon Athena):Amazon Athena is used to query the NBA data directly in S3 using standard SQL.You can run ad hoc queries or create visualizations using tools like Amazon QuickSight. Testing # Test the system by triggering the Lambda function manually or waiting for scheduled events, ensuring that notifications are received correctly via email or SMS.\nConclusion # The Game Day Notification Solution aims to provide seamless updates on NBA games, utilizing AWS\u0026rsquo;s robust infrastructure for scalability and reliability. This project not only enhances user engagement but also demonstrates effective use of cloud technologies in building automated notification systems\n","externalUrl":null,"permalink":"/projects/202501-devops-challenge-ccc/-3/","section":"Projects","summary":"AWS Lambda AWS Glue S3 bucket Amazon Athena SportsData API null Overview # The challenge focuses on building a data lake on AWS to store and analyze NBA data.","title":"3: Data Lake for NBA Analytics","type":"Projects"},{"content":" AWS Lambda AWS SNS S3 bucket EventBridge null Introduction # The Game Day Notification provides solution for timely updates on NBA games through SMS and email notifications.\nUsing an event-driven architecture, the application automatically sends notifications every few hours or at the end of the day, including final scores and relevant game information. It leverages a free Sports Data API to retrieve game data and AWS services for efficient processing and delivery. Project Overview # Goal: Develop a notification app that alerts users about NBA games. Frequency: Notifications will be sent every few hours or at the end of the day. Data Source: SportsDataIO (You can replace this with the API you choose). Notification Method: Email and SMS using Amazon SNS. Architecture and Data Flow # graph TD; A[EventBridge] --\u003e|Triggers| B[AWS Lambda]; B --\u003e|Retrieves Data| C[Sports Data API]; C --\u003e|Returns JSON Data| B; B --\u003e|Transforms Data| D[Human-Readable Format]; D --\u003e|Publishes Message| E[Amazon SNS]; E --\u003e|Notifies Subscribers| F[Email/SMS Subscribers]; subgraph \"AWS Services\" A B E end subgraph \"External Data\" C end subgraph \"User Interaction\" F end The solution employs an event-driven architecture using the following AWS services:\nAWS Lambda: Executes code in response to API requests. Amazon SNS (Simple Notification Service): Sends notifications to subscribers via email or SMS. Amazon EventBridge: Schedules tasks similar to Cron jobs to automate notifications. Workflow # Event Trigger: Amazon EventBridge triggers the AWS Lambda function at scheduled intervals. Data Retrieval: AWS Lambda queries the Sports Data API for game updates. Data Processing: The Lambda function processes the JSON response, extracts relevant game data, and formats it into human-readable messages. Notification Publishing: The Lambda function publishes the formatted messages to an Amazon SNS topic. Subscriber Delivery: Amazon SNS handles the delivery of notifications to subscribers via email or SMS. Testing # Test the system by triggering the Lambda function manually or waiting for scheduled events, ensuring that notifications are received correctly via email or SMS.\nConclusion # The Game Day Notification Solution aims to provide seamless updates on NBA games, utilizing AWS\u0026rsquo;s robust infrastructure for scalability and reliability. This project not only enhances user engagement but also demonstrates effective use of cloud technologies in building automated notification systems.\n","externalUrl":null,"permalink":"/projects/202501-devops-challenge-ccc/-2-game-day-notification/","section":"Projects","summary":"AWS Lambda AWS SNS S3 bucket EventBridge null Introduction # The Game Day Notification provides solution for timely updates on NBA games through SMS and email notifications.","title":"2: Game Day Notification","type":"Projects"},{"content":" Tkinter Requests Boto3 null Overview # The challenge involves building a Python application that fetches real-time weather data from the OpenWeatherMap API and saves it to an AWS S3 bucket. I decided to create a simple graphical user interface (GUI) using Tkinter to make the application more user-friendly.\nFeatures # Fetch Weather Data: Retrieves current weather conditions for a specified city using the OpenWeatherMap API. Display Weather Data: Presents the fetched data in a clear and readable format within the GUI. Save to S3 Bucket: Stores the raw JSON data received from the API into an AWS S3 bucket for later analysis or use. Project Snippets # Technologies Used # Python: The core programming language for the application. Tkinter: Used to create the graphical user interface. Requests: For making HTTP requests to the OpenWeatherMap API. Boto3: The AWS SDK for Python, used to interact with S3. dotenv: To load environment variables (API keys, bucket name) securely. Project Structure # weather-dashboard/ ├── src/ # Source code for the application │ ├── __init__.py # Makes src a package (can be empty) │ ├── gui.py # Tkinter GUI code │ ├── weather_api.py # Weather API interaction logic │ └── s3_utils.py # S3 bucket interaction logic ├── tests/ # Unit tests for the application │ ├── __init__.py │ ├── test_gui.py │ ├── test_weather_api.py │ └── test_s3_utils.py ├── .gitignore # Files/folders to exclude from Git ├── requirements.txt # Project dependencies ├── README.md # Project description and instructions └── main.py # Entry point of the application Architecture Diagram # graph LR subgraph User_Interaction A[User] -.-\u003e B{Enter City} end subgraph Weather_Application M[main.py] --\u003e C M[main.py] --\u003e E M[main.py] --\u003e G B{Enter City} --\"City Name\"--\u003e C[weather_api.py] C[weather_api.py] --\"API Request\"--\u003e D[OpenWeather API] D[OpenWeather API] --\"Weather Data\"--\u003e C C --\"Formatted Data\"--\u003e E[Weather Dashboard GUI] E[Weather Dashboard GUI] --\u003e F[Display Weather] end subgraph Data_Storage C[weather_api.py] -- \"Weather Data\" --\u003e G[s3_utils.py] G[s3_utils.py] --\"Store Data\"--\u003e H[AWS S3] end F[Display Weather] -.-\u003e A[User] %% Styling for better readability style A fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,font-size:16px,color:#1b5e20 style B fill:#ffffff,stroke:#2e7d32,stroke-width:1px,font-size:16px,color:#000000 style C fill:#e1f5fe,stroke:#01579b,stroke-width:2px,font-size:16px,color:#000000 style D fill:#f5f5f5,stroke:#616161,stroke-width:1px,font-size:16px,color:#000000 style E fill:#e1f5fe,stroke:#01579b,stroke-width:2px,font-size:16px,color:#000000 style F fill:#f5f5f5,stroke:#616161,stroke-width:1px,font-size:16px,color:#000000 style G fill:#e1f5fe,stroke:#01579b,stroke-width:2px,font-size:16px,color:#000000 style H fill:#f5f5f5,stroke:#616161,stroke-width:1px,font-size:16px,color:#000000 style M fill:#e1f5fe,stroke:#01579b,stroke-width:2px,font-size:16px,color:#000000 How to Run # Prerequisites:\nPython 3.7+: Install from https://www.python.org/ Libraries: Install required packages: pip install -r requirements.txt OpenWeatherMap API Key: Obtain an API key from https://openweathermap.org/api AWS Account: Create an account on https://aws.amazon.com/ S3 Bucket: Create an S3 bucket in your preferred AWS region. Configuration:\nCreate a .env file in the project\u0026rsquo;s root directory.\nAdd your API key and bucket name to the .env file (see example below):\nOPENWEATHER_API_KEY=your_actual_api_key_here AWS_BUCKET_NAME=your_bucket_name Run the application:\npython main.py Issues and Troubleshooting # This section outlines some common issues I have encountered while setting up or running the weather dashboard application and provides solutions or workarounds.\n1. \u0026ldquo;IllegalLocationConstraintException\u0026rdquo; when Creating S3 Bucket:\nProblem: You might encounter the error \u0026ldquo;An error occurred (IllegalLocationConstraintException) when calling the CreateBucket operation\u0026rdquo; if you haven\u0026rsquo;t specified a region for your S3 bucket or if the default region setting is incompatible.\nSolution:\nSpecify Region in s3_utils.py: Ensure you\u0026rsquo;ve set the correct AWS region in the S3Handler class within src/s3_utils.py. Replace \u0026quot;your-aws-region\u0026quot; with your desired region (e.g., \u0026quot;us-east-1\u0026quot;, \u0026quot;ap-south-1\u0026quot;):\nself.region_name = \u0026#39;your-aws-region\u0026#39; self.s3 = boto3.client(\u0026#39;s3\u0026#39;, region_name=self.region_name) Configure Default AWS Region: For convenience, set your desired region as the default in your AWS CLI or Python environment to avoid specifying it repeatedly.\n2. \u0026ldquo;NoSuchBucket\u0026rdquo; Error when Saving Data:\nProblem: The error \u0026ldquo;An error occurred (NoSuchBucket) when calling the PutObject operation\u0026rdquo; usually means the S3 bucket you\u0026rsquo;re attempting to save data to doesn\u0026rsquo;t exist.\nSolution:\nVerify Bucket Name: Double-check that the bucket name in your code (self.bucket_name) matches the name of the bucket you created in S3. Remember that bucket names are globally unique. Confirm Bucket Creation: Make sure the code to create the bucket (create_bucket_if_not_exists) runs successfully before attempting to save data. This \u0026ldquo;Issues and Troubleshooting\u0026rdquo; section provides a starting point for addressing common problems. If you encounter issues not listed here, refer to the AWS documentation, online forums, or contact the project maintainers for further assistance.\n","externalUrl":null,"permalink":"/projects/202501-devops-challenge-ccc/1-weather-dashboard/","section":"Projects","summary":"Tkinter Requests Boto3 null Overview # The challenge involves building a Python application that fetches real-time weather data from the OpenWeatherMap API and saves it to an AWS S3 bucket.","title":"1: Weather Dashboard","type":"Projects"},{"content":" Blog Post null Project Overview # Get Started with Go is a project aimed at providing a comprehensive introduction to Go programming. The project includes a series of tutorials and code examples designed to help beginners learn the fundamentals of Go and start building their own applications.\nRole and Responsibilities # As the creator of the project, I was responsible for:\nPlanning the curriculum and structure of the tutorials. Writing and testing the Go code examples. Creating detailed documentation to accompany the tutorials. Managing the GitHub repository. Challenges # Designing a curriculum that is both comprehensive and accessible to beginners. Ensuring that the code examples are clear and effective in teaching the intended concepts. Writing thorough documentation to support the tutorials. Solution # Researched and studied various Go learning resources to design an effective curriculum. Developed clear and concise code examples to illustrate key concepts in Go. Wrote detailed documentation and explanations to accompany each tutorial. Key Learnings # Enhanced my understanding of Go through the process of teaching it to others. Improved my skills in creating educational content, blogs and writing clear documentation. Learned how to design a learning path that is accessible and engaging for beginners. Future Improvements # Expand the tutorials to cover more advanced topics in Go. Add more interactive examples and exercises to enhance learning. Create video tutorials to complement the written documentation. ","externalUrl":null,"permalink":"/projects/202308-get-started-with-go/","section":"Projects","summary":"Designed for anyone who is interested in learning Basics of GoLang.","title":"Learn Go Programming Language","type":"Projects"},{"content":" Blog Post null Project Overview # GoVerifyDomain is a tool developed with Golang (Go) in orderto verify domain ownership.\nIt is designed to be a simple, efficient, and reliable way to verify domain ownership by checking DNS records. Challenges # Handling DNS records efficiently. Ensuring the tool could handle various edge cases in domain verification. Solution # Studied Go documentation and best practices to write clean and efficient code. Used Go\u0026rsquo;s net package to handle DNS lookups. Implemented robust error handling and validation to manage edge cases effectively. Key Learnings # Gained a solid understanding of Go programming. Learned how to handle DNS records programmatically. Improved skills in writing clean, maintainable code and documentation. Future Improvements # Add more features such as batch domain verification. Improve the user interface and add more detailed output. Write more comprehensive tests to cover additional edge cases. ","externalUrl":null,"permalink":"/projects/202307-go-verify-domain/","section":"Projects","summary":"A simple domain verification tool written in Go (Golang).","title":"Go Verify Domain","type":"Projects"},{"content":" Blog Post null Project Overview # This project aimed to create a fundamental web server using the Go programming language.\nThe objective was to understand and demonstrate the core concepts of web server development, including handling HTTP requests, routing, and serving static files. Challenges # Designing the overall structure and functionality of the web server. Writing and debugging Go code to handle HTTP requests and responses. Implementing routing to manage different endpoints. Ensuring the web server can serve static files. Effectively handling multiple HTTP requests concurrently. Implementing robust error handling mechanisms. Ensuring the web server performs efficiently under load. Key Learnings # Gained a deeper understanding of Go\u0026rsquo;s capabilities and syntax. Learned to effectively manage concurrent operations using goroutines. Acquired fundamental knowledge of HTTP protocols and web server operations. Developed robust error handling strategies to ensure server reliability. Future Improvements # Implement more advanced routing capabilities. Add security features such as HTTPS, authentication, and authorization. Conduct thorough load testing to evaluate and improve server performance. Integrate features like database connectivity and API endpoints. ","externalUrl":null,"permalink":"/projects/202310-building-a-simple-web-server-with-go/","section":"Projects","summary":"A simple web server implemented in Go, serving static files and handling basic HTTP requests.","title":"Building a Simple Web Server with Go","type":"Projects"},{"content":" null Blog Post Introduction # Automation, development and deployment processes of a microservice-based e-commerce application using Google Kubernetes Engine (GKE) and Google Cloud Platform (GCP). This project demonstrates the challenges faced, the solutions implemented, the technologies used, and provides references and links to the resources leveraged during the project.\nChallenges # Scalability and Maintenance:\nManaging multiple microservices efficiently (11 microservices). Ensuring each service can scale independently. Security:\nConfiguring secure access and permissions for different services. Implementing robust policies for access control. Automation:\nSetting up CI/CD pipelines for each microservice. Automating the deployment process to minimize manual intervention. Infrastructure Management:\nSetting up and managing the Kubernetes cluster on GKE. Autoscaling worker nodes based on demand. Integration:\nIntegrating various tools like Jenkins, Docker, and GKE. Ensuring seamless communication between microservices. Solution # Microservice Setup and CI/CD Pipelines:\nEach microservice was set up with a dedicated CI/CD pipeline. Automated triggers were configured for changes in the GitHub repository. Security Enhancements:\nConfigured VPC (Virtual Private Cloud) and Firewall rules to control access. IAM roles and policies were set up for GKE user, and service account keys were created for authentication. Infrastructure Automation:\nKubernetes cluster was set up using gcloud and kubectl. Infrastructure as Code (IaC) was implemented using Google Cloud Deployment Manager. Autoscaling and Node Management:\nWorker nodes were configured to auto-scale based on demand using GKE\u0026rsquo;s autoscaling features. Persistent Disk volumes and SSH access were set up for efficient management. Tool Integration:\nJenkins was set up with Docker and Kubernetes plugins for pipeline creation. Docker images were built, tagged, and pushed to Google Container Registry (GCR) through Jenkins pipelines. Deployment and Services Configuration:\nDifferent types of Kubernetes services, like NodePort and LoadBalancer, were configured. A microservice deployment service was set up for orchestrating deployments. Technologies Used # References and Links # GKE Documentation Jenkins Pipeline Documentation Kubernetes Documentation gcloud SDK kubectl Installation Guide ","externalUrl":null,"permalink":"/projects/202407-microservices-architecture-on-gke/","section":"Projects","summary":"null Blog Post Introduction # Automation, development and deployment processes of a microservice-based e-commerce application using Google Kubernetes Engine (GKE) and Google Cloud Platform (GCP).","title":"E-commerce Microservices deployment on GKE","type":"Projects"},{"content":" null Overview # The AWS DevOps Challenge is designed to challenge participants to develop and demonstrate their understanding of the core DevOps concepts, tools, and practices. The projects span a variety of DevOps and Cloud domains, including AWS, CI/CD pipelines, infrastructure automation, containerization, and monitoring.\nOfficial Challenge Links # Challenge Discord Community ","externalUrl":null,"permalink":"/projects/202501-devops-challenge-ccc/","section":"Projects","summary":"null Overview # The AWS DevOps Challenge is designed to challenge participants to develop and demonstrate their understanding of the core DevOps concepts, tools, and practices.","title":"AWS DevOps Challenge | Cozy Cloud Crew","type":"Projects"},{"content":" Blog Post null Project Overview # Optimize Docker images using multistage builds.\nThe project involves creating a portfolio site with Hugo and using Docker\u0026rsquo;s multistage build feature to streamline the image creation process, resulting in smaller and more efficient images. Challenges # Creating an efficient Docker image for a Hugo portfolio site posed several challenges:\nLarge Image Size: Traditional Docker images often become bloated with unnecessary dependencies, leading to longer build and deployment times.\nBuild Complexity: Managing dependencies and build tools within a single Dockerfile can complicate the build process and introduce potential errors.\nDeployment Efficiency: Ensuring that the deployment process is streamlined and the final image is optimized for production use.\nSolution # To address these challenges, a multistage Docker build approach was implemented:\nStage 1: Build Stage\nThe first stage focuses on setting up the environment and building the Hugo site. This stage includes all necessary build tools and dependencies. The node:14-alpine image is used to install npm dependencies and build the Hugo site. Stage 2: Production Stage\nThe second stage is optimized for production, copying only the necessary files from the build stage. The nginx:alpine image is used to serve the static files generated by Hugo, ensuring a lightweight and efficient final image. This approach ensures that the final Docker image is significantly smaller, containing only the essential files and dependencies needed to run the Hugo site in production. Key Learnings # Multistage Builds: Gained proficiency in using Docker\u0026rsquo;s multistage build feature to optimize images. Hugo: Learned to effectively use Hugo for static site generation. Image Optimization: Discovered techniques to significantly reduce Docker image sizes. Best Practices: Identified and documented best practices for Docker image optimization. Future Improvements # Additional Optimizations: Explore further optimizations for even smaller and more efficient images. CI/CD Integration: Integrate continuous integration and continuous deployment (CI/CD) pipelines to automate the build and deployment process. Security Enhancements: Implement security best practices to ensure the Docker images are secure. Feature Expansion: Add more features to the portfolio site and explore other use cases for multistage builds. ","externalUrl":null,"permalink":"/projects/202312-hugo-portfolio-multistage-docker/","section":"Projects","summary":"Optimize Docker images for a Hugo website using distroless builds.","title":"Deploying Portfolio with Multistage Docker Builds","type":"Projects"},{"content":" null Blog Post Challenges # Before implementing this monitoring solution, the organization faced several challenges:\nLack of Centralized Monitoring:: Difficulty in tracking the health and performance of multiple systems and services across different environments. Inconsistent Alerting:: Alerts were not standardized, leading to delayed responses to critical issues and increased downtime. Scalability Issues:: Existing monitoring tools could not scale efficiently with the growing infrastructure, leading to performance bottlenecks and incomplete data collection. Manual Infrastructure Management:: Infrastructure setup and management were done manually, resulting in inconsistent environments and potential for human error. Solution # To address these challenges, the team implemented a monitoring system with the following key components:\nKey Components # Prometheus for Centralized Monitoring:: Prometheus was deployed as the core monitoring system to collect and store metrics from various sources, including system resources and application-specific metrics. Node Exporter and Blackbox Exporter:: Node Exporter was used to gather hardware and OS metrics from hosts, while Blackbox Exporter was employed to probe endpoints over multiple protocols. Alertmanager for Real-Time Alerts:: Alertmanager was integrated with Prometheus to manage and route alerts based on predefined rules, ensuring timely notification of critical issues. Infrastructure as Code with Terraform:: Terraform scripts were developed to automate the provisioning and management of the monitoring infrastructure, ensuring consistency and repeatability. Grafana for Data Visualization:: Grafana was utilized to create intuitive dashboards for visualizing the collected metrics, enabling easy monitoring and analysis. Technologies Used # Terraform: To automate the provisioning and management of the monitoring infrastructure. Prometheus: The core monitoring system for collecting and storing metrics. Node Exporter: For collecting hardware and OS metrics from hosts. Blackbox Exporter: To probe endpoints over various protocols such as HTTP, HTTPS, DNS, TCP, and ICMP. Alertmanager: To manage and route alerts based on the metrics collected by Prometheus. References and Links # Project Repository: GitHub - real-time-devops-monitoring Prometheus Documentation: Prometheus Terraform Documentation: Terraform Node Exporter Documentation: Node Exporter Blackbox Exporter Documentation: Blackbox Exporter Alertmanager Documentation: Alertmanager ","externalUrl":null,"permalink":"/projects/202404-realtime-devops-monitoring/","section":"Projects","summary":"null Blog Post Challenges # Before implementing this monitoring solution, the organization faced several challenges:","title":"Realtime Devops solution for Monitoring","type":"Projects"},{"content":" Blog Post null Introduction # This project implements a comprehensive security-centric CI/CD pipeline designed for modern cloud-native applications. Built on Google Cloud Platform (GCP), it demonstrates enterprise-grade security practices and automation throughout the software development lifecycle.\nKey Features # Security by Design:\nMulti-layer security scanning with Aqua Trivy and SonarQube Kubernetes security auditing using Kubeaudit Secure artifact storage with Nexus Repository Automated vulnerability assessments at code and container levels Automation First:\nFully automated pipeline using Jenkins Infrastructure as Code with Terraform Containerized deployments with Docker and Kubernetes Automated quality gates and security checks Comprehensive Monitoring:\nReal-time system metrics with Prometheus Visual dashboards through Grafana Automated alerts via Gmail Blackbox monitoring for external endpoint health Business Benefits # Reduced security risks through automated scanning and continuous monitoring Faster time to market with automated deployment pipeline Improved code quality through automated testing and analysis Enhanced reliability with continuous monitoring and alerting Solution # Workflow # Development:: Developers create feature branches and push code to GitHub. CI/CD Pipeline Trigger:: Code changes trigger the Jenkins CI/CD pipeline. Build and Unit Testing:: Build tool compiles the code and executes unit tests. Code Quality and Security:: SonarQube performs code quality analysis and Aqua Trivy scans for vulnerabilities in code dependencies. Artifact Creation:: A build artifact (e.g., JAR, WAR) is generated. Artifact Publishing:: The artifact is pushed to Nexus Repository. Container Image Build:: Docker creates a container image using the artifact. Image Vulnerability Scan:: Aqua Trivy scans the image for vulnerabilities. Deployment:: If all checks pass, the image is deployed to Kubernetes. Monitoring and Notifications:: Monitoring solutions track system and website health \u0026amp; Emails are sent for deployment status and critical alerts. Tools and Technologies used # Kubernetes: For container orchestration. Jenkins: CI/CD automation. SonarQube: Code quality and security analysis. Aqua Trivy: Vulnerability scanning. Nexus Repository: Artifact storage. Docker and Docker Hub: Containerization and image registry. Kubeaudit: Kubernetes cluster auditing. Grafana and Prometheus: Monitoring and alerting. Terraform: Infrastructure as Code for provisioning and managing cloud infrastructure. GCP: Cloud platform for hosting infrastructure. ","externalUrl":null,"permalink":"/projects/202403-end-to-end-secure-cicd/","section":"Projects","summary":"Blog Post null Introduction # This project implements a comprehensive security-centric CI/CD pipeline designed for modern cloud-native applications.","title":"Secure CI/CD Pipeline","type":"Projects"},{"content":" null Blog Post Introduction # Deploying multiple microservices to Azure Kubernetes Service (AKS) using Azure DevOps for CI/CD and ArgoCD for GitOps-based continuous deployment. The deployment process leverages Azure DevOps for building and deploying the application, and ArgoCD for managing Kubernetes resources in a GitOps manner.\nChallenges # Complexity in Deployment: Ensuring seamless deployment and management of multiple micro-services across different environments. Maintaining Consistency: Keeping the deployed applications in sync with the desired state defined in the Git repository. Scalability and Reliability: Managing resource allocation and scaling the application to handle varying loads without downtime. Efficient CI/CD Pipeline: Automating the build, test, and deployment stages to streamline development workflows and reduce manual interventions. Solution # The architecture consists of the following components:\nMicroservice Application: A sample microservice application. (example-voting-app) Azure Kubernetes Service (AKS): Managed Kubernetes cluster in Azure. Azure DevOps: CI/CD platform for building and deploying the application. ArgoCD: GitOps continuous delivery tool for Kubernetes. Automated Deployment with ArgoCD: Implemented ArgoCD for continuous deployment, ensuring that the applications are always up-to-date and consistent with the Git repository. This automated approach reduced the complexity of managing micro-services and improved deployment reliability. Container Orchestration with AKS: Leveraged Azure Kubernetes Service (AKS) to deploy and manage the micro-services. AKS provided a robust and scalable environment for running the containers, ensuring high availability and efficient resource utilization. CI/CD Pipeline with Azure DevOps: Developed CI/CD pipelines using Azure DevOps to automate the entire development lifecycle, from code commit to deployment. This integration enabled faster development cycles, continuous testing, and streamlined deployment processes. References and Links # ArgoCD Documentation: ArgoCD Azure Kubernetes Service Documentation: AKS Azure DevOps Documentation: Azure DevOps ","externalUrl":null,"permalink":"/projects/202406-argocd-and-aks-microservices-deployment/","section":"Projects","summary":"null Blog Post Introduction # Deploying multiple microservices to Azure Kubernetes Service (AKS) using Azure DevOps for CI/CD and ArgoCD for GitOps-based continuous deployment.","title":"Microservices Deployment on AKS with GitOps","type":"Projects"},{"content":" null Blog Post Project Overview # The Yelcamp Application is a comprehensive platform designed to manage campground locations, user registrations, reviews, and multimedia content. This project aims to implement a robust 3-tier architecture, encompassing the frontend, backend, and database layers, with an automated CI/CD pipeline to streamline development, testing, and deployment processes.\nChallenges # Complex Architecture: Managing a 3-tier application with separate layers for database, backend, and frontend services. Database Integration: Ensuring seamless integration and management of the cloud database within the Kubernetes environment. Security Measures: Implementing robust security protocols for both local and cloud deployments. Automation: Automating the entire CI/CD pipeline from code commit to deployment. Environment Management: Efficiently managing different environments (development, staging, production) and their respective configurations. Dependency Management: Handling application dependencies and environment variables for smooth deployments. Solution # 3-Tier Architecture Implementation: Frontend: Developed with dynamic features allowing creation and management of campground locations with images. Backend: Node.js application handling user registration, camp creation, and reviews. Database: Integrated cloud database for persistent data storage, managed through Kubernetes. Kubernetes Deployment: Set up Kubernetes for deploying the database in Docker containers. Created deployment files for database resources, ensuring smooth volume and service management. Cloud Database Utilization: Used a cloud database platform for easier management of volumes, services, and pods. Reduced manual efforts and enhanced data persistence and accessibility. CI/CD Pipeline Setup: Configured Jenkins and SonarQube for continuous integration and code quality analysis. Implemented Docker for containerization and Kubernetes for orchestration. Automated pipeline stages, including code build, test, and deployment. Security and Environment Management: Provisioned virtual machines with proper security groups and SSH access. Managed environment variables through Kubernetes secrets. Set up Gcloud CLI for GKE cluster access and IAM roles for resource permissions. Technologies Used # References and Links # Node.js Docker Kubernetes Jenkins SonarQube GKE Cloudinary MongoDB ","externalUrl":null,"permalink":"/projects/202407-deploying-and-scaling-3-tier-app/","section":"Projects","summary":"Deploying a YelpCamp application (a full-stack website for campground reviews) across various environments using Cloud DevOps practices.","title":"Deploying and Scaling 3-tier app with GKE","type":"Projects"},{"content":" I’m an engineer at heart with a passion for turning complex challenges into elegant, scalable solutions. My journey began with the Mechanical Engineering, but my curiosity soon led me into the world of IT and Cloud computing—where I discovered the thrill of building, automating, and securing modern infrastructure.\nWith a background in both mechanical engineering and IT, I bring a unique blend of technical expertise and exceptional communication skills to the table.\nCurrently focused on:\nCloud engineering and support: Azure, Google Cloud Platform, AWS Scalable Infrastructure Design: Building systems that grow with your needs Cloud Security and Compliance: Keeping data safe and systems resilient Serverless Computing and Microservices: Modern, efficient application patterns DevOps practices and automation: Docker, Kubernetes, GitHub Actions, Jenkins Check out my Resume \u0026#x2b50;\nResume Let\u0026rsquo;s Connect \u0026#x2728;\nI’m always open to interesting conversations and collaboration. - I\u0026rsquo;d love to hear from you!\nYour Name Email This will help me respond to your query via an email. Message What would you like to discuss?\nSubmit ","externalUrl":null,"permalink":"/about/","section":"Chetan Thapliyal | Building Scalable Cloud Solutions","summary":"I’m an engineer at heart with a passion for turning complex challenges into elegant, scalable solutions.","title":"About","type":"page"},{"content":"","externalUrl":null,"permalink":"/tags/aks/","section":"Tags","summary":"","title":"AKS","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/alert-manager/","section":"Tags","summary":"","title":"Alert Manager","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/aqua-trivy/","section":"Tags","summary":"","title":"Aqua Trivy","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/argocd/","section":"Tags","summary":"","title":"ArgoCD","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/athena/","section":"Tags","summary":"","title":"Athena","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/tags/aws/","section":"Tags","summary":"","title":"AWS","type":"tags"},{"content":"","externalUrl":null,"permalink":"/series/aws-devops-challenge/","section":"Series","summary":"","title":"AWS DevOps Challenge","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/azure/","section":"Tags","summary":"","title":"Azure","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/azure-container-registry/","section":"Tags","summary":"","title":"Azure Container Registry","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/azure-devops/","section":"Tags","summary":"","title":"Azure DevOps","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/blackbox-exporter/","section":"Tags","summary":"","title":"Blackbox Exporter","type":"tags"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/authors/chetan-thapliyal/","section":"Authors","summary":"","title":"Chetan Thapliyal","type":"authors"},{"content":" Welcome to my personal website! Feel free to reach out to me directly by email or message with any questions or interesting opportunities!\n","externalUrl":null,"permalink":"/","section":"Chetan Thapliyal | Building Scalable Cloud Solutions","summary":"Welcome to my personal website!","title":"Chetan Thapliyal | Building Scalable Cloud Solutions","type":"page"},{"content":"","externalUrl":null,"permalink":"/tags/concurrency/","section":"Tags","summary":"","title":"Concurrency","type":"tags"},{"content":"Let\u0026rsquo;s Connect \u0026#x2728;\nI’m always open to interesting conversations and collaboration. - I\u0026rsquo;d love to hear from you!\nYour Name Email This will help me respond to your query via an email. Message What would you like to discuss?\nSubmit ","externalUrl":null,"permalink":"/contact/","section":"Chetan Thapliyal | Building Scalable Cloud Solutions","summary":"Let\u0026rsquo;s Connect \u0026#x2728;","title":"Contact","type":"page"},{"content":"","externalUrl":null,"permalink":"/tags/distroless/","section":"Tags","summary":"","title":"Distroless","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/dmarc/","section":"Tags","summary":"","title":"DMARC","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/dns/","section":"Tags","summary":"","title":"DNS","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/docker-hub/","section":"Tags","summary":"","title":"Docker Hub","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/documentation/","section":"Tags","summary":"","title":"Documentation","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/domain-verification/","section":"Tags","summary":"","title":"Domain Verification","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/eventbridge/","section":"Tags","summary":"","title":"EventBridge","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/gcp/","section":"Tags","summary":"","title":"GCP","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/gke/","section":"Tags","summary":"","title":"GKE","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/glue/","section":"Tags","summary":"","title":"Glue","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/go/","section":"Tags","summary":"","title":"Go","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/golang/","section":"Tags","summary":"","title":"Golang","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/grafana/","section":"Tags","summary":"","title":"Grafana","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/http/","section":"Tags","summary":"","title":"HTTP","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/jenkins/","section":"Tags","summary":"","title":"Jenkins","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/kubeaudit/","section":"Tags","summary":"","title":"Kubeaudit","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/lambda/","section":"Tags","summary":"","title":"Lambda","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/learn/","section":"Tags","summary":"","title":"Learn","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/mongodb/","section":"Tags","summary":"","title":"MongoDB","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/multistage/","section":"Tags","summary":"","title":"Multistage","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/mx-records/","section":"Tags","summary":"","title":"MX records","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/nexus-repository/","section":"Tags","summary":"","title":"Nexus Repository","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/node-exporter/","section":"Tags","summary":"","title":"Node Exporter","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/node.js/","section":"Tags","summary":"","title":"Node.js","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/progress-30/","section":"Tags","summary":"","title":"Progress: 30%","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/progress-75/","section":"Tags","summary":"","title":"Progress: 75%","type":"tags"},{"content":"I\u0026rsquo;m continually carving out moments to delve into new projects and expand my knowledge horizons. While many of these passion projects may not materialize fully, they serve as invaluable opportunities for hands-on experimentation and growth in the real-world arena.\n","externalUrl":null,"permalink":"/projects/","section":"Projects","summary":"I\u0026rsquo;m continually carving out moments to delve into new projects and expand my knowledge horizons.","title":"Projects","type":"Projects"},{"content":"","externalUrl":null,"permalink":"/tags/prometheus/","section":"Tags","summary":"","title":"Prometheus","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":" Download Experience Company Link Role Dates Location Hashnode Technical Writer (Freelance) 2023 - Present Remote Chegg India Subject Matter Expert (Mechanical) (Freelance \u0026 Vendors) 2020 - 2023 Remote Report India Founder \u0026 Editor-in-Chief 2018 - 2020 Noida, IN Thapliyal Book Depot Business Manager 2014 - 2017 Pokhari, IN Education Formal Education School Link Degree Date Location Uttarakhand Technical University Bachelors of Technology (Mechanical) 2010 - 2014 Dehradun, IN Informal Education Training Issued by Certification Time Google Google IT Support Professional Certificate 8 months The Linux Foundation LFS207: Linux System Administration Essentials 6 months IBM Data Science Professional Certificate 1 year Microsoft Applied Skills Applied Skills Year Secure storage for Azure Files and Azure Blob Storage 2024 Certifications Certification Provider Year Google Cloud: Associate Cloud Engineer (Working on it) Google upcoming in 2025 Microsoft Certified: Azure AI Fundamentals Microsoft 2025 Credly Verification ","externalUrl":null,"permalink":"/resume/","section":"Chetan Thapliyal | Building Scalable Cloud Solutions","summary":"With a diverse skill set encompassing technical writing, communication, project management, and a wide array of cloud technologies, I am poised to contribute to and thrive in the ever-evolving landscape of cloud engineering and support.","title":"Resume","type":"page"},{"content":"","externalUrl":null,"permalink":"/tags/running/","section":"Tags","summary":"","title":"Running","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/s3/","section":"Tags","summary":"","title":"S3","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/series/","section":"Tags","summary":"","title":"Series","type":"tags"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/sns/","section":"Tags","summary":"","title":"SNS","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/sonarqube/","section":"Tags","summary":"","title":"SonarQube","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/spf-records/","section":"Tags","summary":"","title":"SPF records","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/static-files/","section":"Tags","summary":"","title":"Static Files","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/terraform/","section":"Tags","summary":"","title":"Terraform","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/trivy/","section":"Tags","summary":"","title":"Trivy","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/tutorial/","section":"Tags","summary":"","title":"Tutorial","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/under-construction/","section":"Tags","summary":"","title":"Under Construction","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/web-server/","section":"Tags","summary":"","title":"Web Server","type":"tags"}]